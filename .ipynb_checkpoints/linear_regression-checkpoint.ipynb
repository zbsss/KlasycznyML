{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMa7uVQOGdvz"
   },
   "source": [
    "# Regresja liniowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJ3hmmkzGdvz"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICOFOAFXGdv2"
   },
   "source": [
    "## Zbiór danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sT2ph-ZaGdv3"
   },
   "source": [
    "Zajmiemy się analizą danych wiążących śmiertelność chorób nowotworowych w poszczególnych hrabstwach USA z danymi demograficznymi z tych hrabstw. Ponieważ zbiór zawiera bardzo dużo zmiennych opisujących, co utrudnia wizualizację, w celach dydaktycznych wybrano tylko kilka z nich:\n",
    "\n",
    "* **PctPublicCoverageAlone** - mieszkańcy hrabstwa z ubezpieczeniem zdrowotnym pokrywanym przez państwo (wyrażone w procentach)\n",
    "* **povertyPercent** - mieszkańcy hrabstwa, którzy żyją w ubóstwie (wyrażone w procentach)\n",
    "* **AvgHouseholdSize** - średni rozmiar gospodarstwa domowego w hrabstwie\n",
    "* **PctUnemployed16_Over** - mieszkańcy hrabstwa powyżej 16 roku życia, którzy sa bezrobotni (wyrażone w procentach)\n",
    "* **PctHS18_24** - mieszkańcy hrabstwa w wieku 18-24 lata z wykształceniem średnim lub niższym (wyrażone w procentach)\n",
    "\n",
    "Zmienna opisywana:\n",
    "\n",
    "* **TARGET_deathRate** - średnia liczba osób na 100 000 mieszkańców, które umarły z powodu chorób nowotworowych w latach 2010-2016 \n",
    "\n",
    "Źródło zbioru danych: https://data.world/nrippner/ols-regression-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dw6zCgVGdv3"
   },
   "outputs": [],
   "source": [
    "cancer_data = pd.read_csv(\"cancer_reg.csv\")[[\"TARGET_deathRate\", \"PctPublicCoverageAlone\", \"povertyPercent\", \"AvgHouseholdSize\", \"PctUnemployed16_Over\", \"PctHS18_24\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfTN2REAGdv6"
   },
   "outputs": [],
   "source": [
    "cancer_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXk-MGjWGdv8"
   },
   "outputs": [],
   "source": [
    "cancer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2rNgwtpGdv-"
   },
   "outputs": [],
   "source": [
    "cancer_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRasHz5PGdwB"
   },
   "outputs": [],
   "source": [
    "cancer_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2rX9pPVGdwC"
   },
   "source": [
    "Zwizualizujmy, jak poszczególne zmienne w modelu zależą od siebie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vn7o0X-TGdwD"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(cancer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtxonOtxGdwF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(cancer_data[\"TARGET_deathRate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4KkSEIiGdwH"
   },
   "outputs": [],
   "source": [
    "sns.heatmap(cancer_data.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUgF6AE5GdwJ"
   },
   "source": [
    "Można wywnioskować, że zmienne opisujące nie są silnie skorelowane ze sobą nawzajem (może z wyjątkiem povertyPercent i PctPublicCoverageAlone, ale z tym regresja powinna sobie jeszcze poradzić). Jest to jedyne założenie o danych potrzebne do regresji liniowej, które możemy prosto sprawdzić bez posiadania już wyliczonego modelu.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRis_xlNGdwK"
   },
   "source": [
    "## Regresja liniowa ze Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmKD9pSvGdwK"
   },
   "source": [
    "W tej części sprawdzimy jak gotowy model regresji liniowej z pakietu Scikit-learn poradzi sobie z zadaniem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDb8iiQOGdwK"
   },
   "source": [
    "### Podział danych i model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6yyJjfkGdwL"
   },
   "outputs": [],
   "source": [
    "X = cancer_data[[\n",
    "    \"PctPublicCoverageAlone\",\n",
    "    \"povertyPercent\",\n",
    "    \"AvgHouseholdSize\",\n",
    "    \"PctUnemployed16_Over\",\n",
    "    \"PctHS18_24\"]]\n",
    "\n",
    "y = cancer_data[\"TARGET_deathRate\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqCIDQIXGdwN"
   },
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3OKdCbcGdwP"
   },
   "source": [
    "### Działanie modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJbR2OqsGdwP"
   },
   "source": [
    "Zobaczmy, jakie wartości przymują dla naszego modelu metryki zdefiniowane na wykładzie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iV0Ng-2SGdwP"
   },
   "outputs": [],
   "source": [
    "print(f\"RSS: {np.sum((y_test - linear_model.predict(X_test)) ** 2)}\")\n",
    "print(f\"RMSE: {np.sqrt(np.mean((y_test - linear_model.predict(X_test)) ** 2))}\")\n",
    "print(f\"MAE: {np.mean(abs(y_test - linear_model.predict(X_test)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XaToW_LHGdwS"
   },
   "outputs": [],
   "source": [
    "print(linear_model.intercept_)\n",
    "coeff_df = pd.DataFrame(linear_model.coef_, X.columns, columns=[\"Coefficient\"])\n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mncdN1uFGdwT"
   },
   "outputs": [],
   "source": [
    "predictions = linear_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGJD4bUfGdwW"
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_test, linear_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4pCq0H2GdwY"
   },
   "outputs": [],
   "source": [
    "sns.distplot((y_test-linear_model.predict(X_test)), bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6tZsdOfGdwa"
   },
   "source": [
    "Widzimy, że szacowany błąd na zbiorze testowym jest dany rozkładem normalnym o wartości oczekiwanej w przybliżeniu równej 0. Zatem w tym momencie możemy stwierdzić, iż spełnione jest kolejne z założeń o danych, potrzebne dla właściwego funkcjonowania regresji liniowej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnXwiNs6Gdwa"
   },
   "source": [
    "### Ustandaryzowanie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3_KZ6ajGdwb"
   },
   "source": [
    "Sprawdzimy, jak ustandaryzowanie danych tak, aby miały tę samą średnią i odchylenie standardowe wpłynie na wynik regresji. W tym celu ponownie zastosujemy pakiet Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQe9WCatGdwb"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_rescaled = scaler.transform(X_train)\n",
    "X_test_rescaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsZtcJKlGdwd"
   },
   "outputs": [],
   "source": [
    "linear_model_r = LinearRegression()\n",
    "linear_model_r.fit(X_train_rescaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzCnrahdGdwf"
   },
   "outputs": [],
   "source": [
    "print(f\"RSS: {np.sum((y_test - linear_model_r.predict(X_test_rescaled)) ** 2)}\")\n",
    "print(f\"RMSE: {np.sqrt(np.mean((y_test - linear_model_r.predict(X_test_rescaled)) ** 2))}\")\n",
    "print(f\"MAE: {np.mean(abs(y_test - linear_model_r.predict(X_test_rescaled)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ha-2opLgGdwh"
   },
   "outputs": [],
   "source": [
    "print(linear_model_r.intercept_)\n",
    "coeff_df = pd.DataFrame(linear_model_r.coef_, X.columns, columns=[\"Coefficient\"])\n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpgZKbkkGdwi"
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_test, linear_model_r.predict(X_test_rescaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrWWmd9zGdwk"
   },
   "outputs": [],
   "source": [
    "sns.distplot((y_test-linear_model_r.predict(X_test_rescaled)), bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEcOkXDzGdwm"
   },
   "source": [
    "Możemy zauważyć, że metryki nie zmieniają się, mimo że trochę zmieniliśmy dane, a co za tym idzie i wyliczone współczynniki.\n",
    "\n",
    "Mimo że nie zmieniły sie wartości metryk, widzimy korzystną zmianę wśród wyliczonych wartości współczynników - są one teraz tego samego rzędu wielkości, co zapobiega potencjalnym błędom numerycznym. Ponadto po tej zmianie, wyliczony wyraz wolny modelu możemy interpretować jako wartość przyjmowaną przez funkcję, gdy dane wejściowe przyjmują swoje wartości średnie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6fmB-pSGdwm"
   },
   "source": [
    "## Prosta regresja liniowa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWjZULzcGdwm"
   },
   "source": [
    "Zanim przejdziemy do głównego zadania, spróbujmy czegoś prostszego - znalezienia prostej najlepszego dopasowania dla sztucznie wygenerowanych jednowymiarowych danych.\n",
    "\n",
    "Wygenerujemy dane dla równania: $$y = 2x+1 + \\varepsilon $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2vY_aGyGdwn"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, num=100).reshape(100, 1)\n",
    "y = 2. * x + 1 + np.random.normal(size=(100, 1))\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioLtk56dGdwq"
   },
   "source": [
    "Teraz zapomnijmy na chwilę, że znamy zadane współczynniki i spróbujmy znaleźć je przy pomocy prostej regresji liniowej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8F22p3Gdwq"
   },
   "source": [
    "### Prosta regresja liniowa z zejściem wzdłuż gradientu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycvW7GRzGdwq"
   },
   "source": [
    "Przypomnijmy z wykładu,że prostą najlepszego dopasowania znajdujemy poprzez minimalizację pewnej funkcji kosztu (uznajemy, że zmiennymi w tej funkcji są współczynniki równania, którego szukamy).\n",
    "\n",
    "Wektory $x$ i $y$ są wymiaru $n$ $x$ $1$.\n",
    "\n",
    "Zdefiniujmy naszą funkcję kosztu:\n",
    "\n",
    "$$L(w_0, w_1) = \\frac{1}{2n}\\sum_{i=0}^{n-1}(w_0+w_1x_i-y_i)^2$$\n",
    "\n",
    "Różni się ona nieco od tej zdefiniowanego na wykładzie RSS:\n",
    "* Podzielimy wartość przez 2, aby uprościć stałą pochodzącą z kwadratu w pochodnej.\n",
    "* Podzielimy wartość przez $n$, aby uczynić funkcję kosztu niezależną od liczby analizowanych w danym momencie próbek.\n",
    "\n",
    "Naszym pierwszym podejściem będzie minimalizacja funkcji kosztu poprzez zejście wzdłuż gradientu. Wykonamy pewną liczbę iteracji (liczbę iteracji będzie zadawał parametr epoch_number). W każdej iteracji wyliczymy pochodne po $w_0$ oraz $w_1$, a następnie zaktualizujemy wagi odejmując od nich odpowiadające im pochodne pomnożone przez niewielką stałą uczącą (parametr nazwany lr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DD--N-zvGdwr"
   },
   "outputs": [],
   "source": [
    "class SimpleLinearModelGD():\n",
    "    def __init__(self):\n",
    "        self.w0 = np.random.rand()\n",
    "        self.w1 = np.random.rand()\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Computes model prediction for given datapoints.\n",
    "        \n",
    "        n - number of data samples\n",
    "        \n",
    "        Args:\n",
    "          x: Vector of datapoints. Size: n x 1.\n",
    "          \n",
    "        Returns:\n",
    "          Predicted y values.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Implement \n",
    "        raise NotImplementedError(\"Code not implemented\")\n",
    "        #\n",
    "\n",
    "    def loss_function(self, x, y):\n",
    "        \"\"\"\n",
    "        Computes loss function.\n",
    "        \n",
    "        n - number of data samples\n",
    "        p - number of model coefficients including the intercept\n",
    "        \n",
    "        Args:\n",
    "          x: Vector of datapoints. Size: n x 1.\n",
    "          y: Vector of actual values for given datapoints. Size: n x 1.\n",
    "          \n",
    "        Returns:\n",
    "          Loss function for given data and current coefficients values.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Implement \n",
    "        raise NotImplementedError(\"Code not implemented\")\n",
    "        #\n",
    "\n",
    "    \n",
    "    def dL_dw0(self, x, y):\n",
    "        \"\"\"\n",
    "        Computes derivative of loss function.\n",
    "        \n",
    "        n - number of data samples\n",
    "        \n",
    "        Args:\n",
    "          x: Vector of datapoints. Size: n x 1.\n",
    "          y: Vector of actual values for given datapoints. Size: n x 1.\n",
    "          \n",
    "        Returns:\n",
    "          Derivative of loss function.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Implement \n",
    "        raise NotImplementedError(\"Code not implemented\")\n",
    "        #\n",
    "        \n",
    "    def dL_dw1(self, x, y):\n",
    "        \"\"\"\n",
    "        Computes derivative of loss function.\n",
    "        \n",
    "        n - number of data samples\n",
    "        \n",
    "        Args:\n",
    "          x: Vector of datapoints. Size: n x 1.\n",
    "          y: Vector of actual values for given datapoints. Size: n x 1.\n",
    "          \n",
    "        Returns:\n",
    "          Derivative of loss function.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Implement \n",
    "        raise NotImplementedError(\"Code not implemented\")\n",
    "        #\n",
    "    \n",
    "    def train(self, x, y, lr=.001, epoch_num=20):\n",
    "        \"\"\"\n",
    "        Performs gradient descent.\n",
    "        \n",
    "        n - number of data samples\n",
    "        p - number of model coefficients including the intercept\n",
    "        \n",
    "        Args:\n",
    "          x: Vector of datapoints. Size: n x 1.\n",
    "          y: Vector of actual values for given datapoints. Size: n x 1.\n",
    "          lr: Learning rate.\n",
    "          epoch_num: Number of epochs.\n",
    "        \"\"\"\n",
    "        \n",
    "        loss = []\n",
    "        for i in range(epoch_num):\n",
    "            # Implement \n",
    "            raise NotImplementedError(\"Code not implemented\")\n",
    "            #\n",
    "            loss.append(self.loss_function(x, y))\n",
    "    \n",
    "        plt.plot(loss)\n",
    "        plt.title(\"Loss function\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBsM_nMPGdwt"
   },
   "outputs": [],
   "source": [
    "lr = .001\n",
    "epoch_num = 200\n",
    "np.random.seed(seed=1234)\n",
    "simple_linear_model_gd = SimpleLinearModelGD()\n",
    "simple_linear_model_gd.train(x, y, lr, epoch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDL1F2hEGdwv"
   },
   "outputs": [],
   "source": [
    "print(f\"Computed w_0: {simple_linear_model_gd.w0}\")\n",
    "print(f\"Computed w_0: {simple_linear_model_gd.w1}\")\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, simple_linear_model_gd.predict(x), color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uESTsc6TGdwx"
   },
   "outputs": [],
   "source": [
    "print(f\"RSS: {np.sum((y - simple_linear_model_gd.predict(x)) ** 2)}\")\n",
    "print(f\"RMSE: {np.sqrt(np.mean((y - simple_linear_model_gd.predict(x)) ** 2))}\")\n",
    "print(f\"MAE: {np.mean(abs(y - simple_linear_model_gd.predict(x)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqhCV2IlGdwy"
   },
   "source": [
    "### Prosta regresja liniowa zgodnie ze wzorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXspk3PoGdwz"
   },
   "outputs": [],
   "source": [
    "class SimpleLinearModel():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.w0 = None\n",
    "        self.w1 = None\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Computes model prediction for given datapoints.\n",
    "        \n",
    "        n - number of data samples\n",
    "       \n",
    "        Args:\n",
    "          x: Vector of datapoints. Size: n x 1.\n",
    "          \n",
    "        Returns:\n",
    "          Predicted y values.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Implement \n",
    "        raise NotImplementedError(\"Code not implemented\")\n",
    "        #\n",
    "    \n",
    "    def compute(self, x, y):\n",
    "        \"\"\"\n",
    "        Computes coefficients that give the minimal loss value.\n",
    "        \n",
    "        n - number of data samples\n",
    "       \n",
    "        Args:\n",
    "          x: Vector of datapoints. Size: n x 1.\n",
    "          y: Vector of actual values for given datapoints. Size: n x 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Implement \n",
    "        raise NotImplementedError(\"Code not implemented\")\n",
    "        #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDSgEbiRGdw0"
   },
   "outputs": [],
   "source": [
    "simple_linear_model = SimpleLinearModel()\n",
    "simple_linear_model.compute(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_PYiQHXGdw2"
   },
   "outputs": [],
   "source": [
    "print(f\"Computed w_0: {simple_linear_model.w0}\")\n",
    "print(f\"Computed w_1: {simple_linear_model.w1}\")\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, simple_linear_model.predict(x), color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlyuQWgBGdw3"
   },
   "outputs": [],
   "source": [
    "print(f\"RSS: {np.sum((y - simple_linear_model.predict(x)) ** 2)}\")\n",
    "print(f\"RMSE: {np.sqrt(np.mean((y - simple_linear_model.predict(x)) ** 2))}\")\n",
    "print(f\"MAE: {np.mean(abs(y-simple_linear_model.predict(x)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyB-HYJHGdw5"
   },
   "source": [
    "## Wielowymiarowa regresja liniowa z zejściem wzdłuż gradientu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVpGjRzTGdw5"
   },
   "source": [
    "Wróćmy teraz do naszego zbioru danych.\n",
    "\n",
    "Przypomnijmy, że do obliczenia regresji liniowej potrzebujemy reprezentować dane opisujące funkcję w macierzy o rozmiarze $n$ $x$ $(p-1)$, gdzie $n$ - liczba punktów danych, $p-1$ - wymiar punktów danych. Macierz tę powinniśmy uzupełnić o kolumnę złożoną z samych jedynek (zatem finalnie ma ona rozmiar $n$ $x$ $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwqwwAbGGdw6"
   },
   "source": [
    "### Przygotowanie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agF5Eu5QGdw6"
   },
   "source": [
    "Przygotujmy dane do obliczeń. W tym celu najpierw wykonajmy ich standaryzację, a potem zadbamy o odpowiedni wymiar macierzy i wektorów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23yNOvB8Gdw6"
   },
   "outputs": [],
   "source": [
    "def prepare_data(X_train, y_train, X_test, y_test):\n",
    "    X_train_np = X_train.to_numpy()\n",
    "    X_train_std = np.std(X_train_np, axis=0, keepdims=True)\n",
    "    X_train_mean = np.mean(X_train_np, axis=0, keepdims=True)\n",
    "    X_train_normalized = (X_train_np - X_train_mean) / X_train_std\n",
    "    \n",
    "    X_test_np = X_test.to_numpy()\n",
    "    X_test_normalized = (X_test_np - X_train_mean) / X_train_std\n",
    "    return (np.hstack([np.ones((X_train_np.shape[0], 1)), X_train_normalized]),\n",
    "            np.expand_dims(y_train.to_numpy(), axis=1),\n",
    "            np.hstack([np.ones((X_test_np.shape[0], 1)), X_test_normalized]),\n",
    "            np.expand_dims(y_test.to_numpy(), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvHbBc3jGdw8"
   },
   "outputs": [],
   "source": [
    "X_train_np, y_train_np, X_test_np, y_test_np = prepare_data(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1z-IzSaEGdw-"
   },
   "source": [
    "### Definicja modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55A6jiJpGdw-"
   },
   "outputs": [],
   "source": [
    "class LinearModelGD():\n",
    "    def __init__(self, size):\n",
    "        self.w = np.random.rand(size, 1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Computes model prediction for given datapoints.\n",
    "        \n",
    "        n - number of data samples\n",
    "        p - number of model coefficients including the intercept\n",
    "        \n",
    "        Args:\n",
    "          X: Matrix of datapoints. Size: n x p.\n",
    "          \n",
    "        Returns:\n",
    "          Predicted y values.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Implement \n",
    "        raise NotImplementedError(\"Code not implemented\")\n",
    "        #\n",
    "\n",
    "    def loss_function(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes Residual Sum of Squares.\n",
    "        \n",
    "        n - number of data samples\n",
    "        p - number of model coefficients including the intercept\n",
    "        \n",
    "        Args:\n",
    "          X: Matrix of datapoints. Size: n x p.\n",
    "          y: Vector of actual values for given datapoints. Size: n x 1.\n",
    "          \n",
    "        Returns:\n",
    "          RSS for given data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Implement \n",
    "        raise NotImplementedError(\"Code not implemented\")\n",
    "        #\n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes gradient pf loss function.\n",
    "        \n",
    "        n - number of data samples\n",
    "        p - number of model coefficients including the intercept\n",
    "        \n",
    "        Args:\n",
    "          X: Matrix of datapoints. Size: n x p.\n",
    "          y: Vector of actual values for given datapoints. Size: n x 1.\n",
    "          \n",
    "        Returns:\n",
    "          Gradient of loss function. Size: p x 1\n",
    "        \"\"\"\n",
    "        \n",
    "        # Implement \n",
    "        raise NotImplementedError(\"Code not implemented\")\n",
    "        #\n",
    "    \n",
    "    def train(self, X, y, lr=.001, epoch_num=20000):\n",
    "        \"\"\"\n",
    "        Performs gradient descent.\n",
    "        \n",
    "        n - number of data samples\n",
    "        p - number of model coefficients including the intercept\n",
    "        \n",
    "        Args:\n",
    "          X: Matrix of datapoints. Size: n x p.\n",
    "          y: Vector of actual values for given datapoints. Size: n x 1.\n",
    "          lr: Learning rate.\n",
    "          epoch_num: Number of epochs.\n",
    "        \"\"\"\n",
    "        \n",
    "        loss = []\n",
    "        for i in range(epoch_num):\n",
    "            # Implement \n",
    "            raise NotImplementedError(\"Code not implemented\")\n",
    "            #\n",
    "            loss.append(self.loss_function(X, y))\n",
    "            plt.title(\"Loss function\")\n",
    "    \n",
    "        plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Lw3knNbGdxA"
   },
   "outputs": [],
   "source": [
    "lr = .001\n",
    "epoch_num = 20000\n",
    "np.random.seed(seed=1234)\n",
    "linear_model_gd = LinearModelGD(6)\n",
    "linear_model_gd.train(X_train_np, y_train_np, lr, epoch_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHrwNb_5GdxB"
   },
   "source": [
    "### Działanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HVTT-1DGdxB"
   },
   "outputs": [],
   "source": [
    "linear_model_gd.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bRzzRlaGdxE"
   },
   "outputs": [],
   "source": [
    "print(f\"RSS: {np.sum((y_test_np - linear_model_gd.predict(X_test_np)) ** 2)}\")\n",
    "print(f\"RMSE: {np.sqrt(np.mean((y_test_np - linear_model_gd.predict(X_test_np)) ** 2))}\")\n",
    "print(f\"MAE: {np.mean(abs(y_test_np - linear_model_gd.predict(X_test_np)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3gPVTJcZGdxG"
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_test_np, linear_model_gd.predict(X_test_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zp04LPB2GdxI"
   },
   "outputs": [],
   "source": [
    "sns.distplot((y_test_np-linear_model_gd.predict(X_test_np)), bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaqHfue8GdxK"
   },
   "source": [
    "## Wielowymiarowa regresja liniowa ze wzoru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WW3YWm4oGdxK"
   },
   "source": [
    "### Definicja modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1Y-VzXSGdxK"
   },
   "outputs": [],
   "source": [
    "class LinearModel():\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        self.w = None\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Computes model prediction for given datapoints.\n",
    "        \n",
    "        n - number of data samples\n",
    "        p - number of model coefficients including the intercept\n",
    "        \n",
    "        Args:\n",
    "          X: Matrix of datapoints. Size: n x p.\n",
    "          \n",
    "        Returns:\n",
    "          Predicted y values.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Implement \n",
    "        raise NotImplementedError(\"Code not implemented\")\n",
    "        #\n",
    "    \n",
    "    def compute(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes coefficients that give the minimal loss value.\n",
    "        \n",
    "        n - number of data samples\n",
    "        p - number of model coefficients including the intercept\n",
    "        \n",
    "        Args:\n",
    "          X: Matrix of datapoints. Size: n x p.\n",
    "          y: Vector of actual values for given datapoints. Size: n x 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Implement \n",
    "        raise NotImplementedError(\"Code not implemented\")\n",
    "        #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XrWlRA-uGdxN"
   },
   "outputs": [],
   "source": [
    "linear_model = LinearModel(6)\n",
    "linear_model.compute(X_train_np, y_train_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yj8DpdK_GdxO"
   },
   "source": [
    "### Działanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqNvlmD8GdxO"
   },
   "outputs": [],
   "source": [
    "print(linear_model.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUDeG8uzGdxQ"
   },
   "outputs": [],
   "source": [
    "print(f\"RSS: {np.sum((y_test_np - linear_model.predict(X_test_np)) ** 2)}\")\n",
    "print(f\"RMSE: {np.sqrt(np.mean((y_test_np - linear_model.predict(X_test_np)) ** 2))}\")\n",
    "print(f\"MAE: {np.mean(abs(y_test_np - linear_model.predict(X_test_np)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcYBNGqgGdxR"
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_test_np, linear_model.predict(X_test_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgQnP62-GdxT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot((y_test_np-linear_model.predict(X_test_np)), bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6G3IDy6GdxV"
   },
   "source": [
    "Jak widać obydwoma metodami osiągnęliśmi wynik zgodny z wynikiem danym przez pakiet Scikit-learn."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
